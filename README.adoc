= Key-Based (Keyed) Locking: Challenges & Best Practices

When you build a *key-based lock* (a “keyed semaphore/lock”: only operations with the same key exclude each other), the hard part isn’t mutual exclusion — it’s everything around *lifecycle, correctness under races, cancellation, and scalability*, without turning your lock manager into a bottleneck or a memory leak.

Below are the *main challenges* and *best practices* to solve them, mapped directly to the kind of implementation you uploaded
(*AsyncKeyedLocker + dictionary + per-key `SemaphoreSlim`*).

== Components (as referenced)

* AsyncKeyedLocker
* AsyncKeyedLockDictionary
* AsyncKeyedLockReleaser

== Reentrancy behavior

=== The challenge 

NOTE: This code implements a *non-reentrant asynchronous keyed lock*.

A reentrancy scenario occurs when the *same logical async flow* attempts to
acquire the same key more than once before releasing it.

Because the lock is implemented using a per-key `SemaphoreSlim` with a
maximum count of 1, a second acquisition for the same key will wait until
the first one is released. If both acquisitions happen in the same async
flow, this results in a self-deadlock.

[source,csharp]
----
await using (await locker.LockAsync("K", ct))
{
    // Lock for "K" is held here

    await using (await locker.LockAsync("K", ct))
    {
        // This code is never reached
    }
}
----

The implementation does not track ownership or recursion depth and therefore
cannot distinguish between different callers and the same caller re-entering
the lock.

---

=== Best practices

To avoid self-deadlocks and undefined behavior:

* Do not acquire the same key recursively within the same async call chain
* Acquire the keyed lock at the lowest layer that actually requires exclusivity
* Avoid locking at orchestration or coordination layers
* Pass already-acquired data downward instead of re-acquiring the lock
* Treat the keyed lock as a per-key async mutex, not as a reentrant monitor

If reentrancy detection is required, it should be implemented explicitly on
top of this library (for example, by detecting re-entry and throwing an
exception).

---

=== Why this matters

Supporting reentrancy in async code requires:

* Tracking logical ownership across async continuations
* Flowing context via `AsyncLocal`
* Maintaining recursion depth per key
* Releasing the underlying semaphore only at the outermost scope

This significantly increases implementation complexity, memory overhead,
and the risk of subtle leaks or correctness bugs.

For common use cases such as file IO, blob caching, and per-key write
serialization, reentrancy usually indicates a design issue rather than a
legitimate requirement.

By remaining non-reentrant, this library favors:

* Simplicity and predictability
* Clear usage semantics
* Lower overhead under high concurrency
* Safer lifecycle management of per-key lock objects


== Race conditions between “get lock object” and “remove lock object”

=== The challenge

Two things happen concurrently:

* *Thread A*: “I’m done with key `K`, release and maybe remove it from the dictionary.”
* *Thread B*: “I need key `K`, give me the per-key semaphore.”

If you remove the entry too early, thread B might:

* reuse an object that is being disposed, or
* create *two different semaphores for the same key* (breaking mutual exclusion).

=== Best practices

* *Use a reference count (or equivalent lease) per dictionary entry*
** Increment when a caller obtains the per-key object
** Decrement on release
** Remove from dictionary only when the count reaches zero

+
✔ Your code does this with `ReferenceCount`, removing only when
`ReferenceCount == 1` inside a critical section.

* *Guard “in use vs removed” with a state flag*
** `IsNotInUse` prevents resurrecting a lock that has been logically removed.

* *Make increment + “still valid?” atomic*
** Implemented via `Monitor.TryEnter(this)` inside `TryIncrement()`
** Atomically checks `IsNotInUse` and increments the count

=== Why this matters

If you get this wrong, you don’t just get a crash — you get *silent correctness bugs*:

* two operations with the same key run concurrently,
* corrupted files, caches, DB rows, or blob objects.

== Cancellation: avoiding refcount leaks and “stuck forever” entries

=== The challenge

Flow:

. Get or create per-key semaphore object (*refcount++*)
. `await WaitAsync(...)`

If waiting is *cancelled*, the semaphore was never acquired —
but the refcount was incremented.

If you don’t fix this:

* dictionary entries remain forever,
* memory grows,
* “dead keys” accumulate (especially with high-cardinality keys).

=== Best practices

* *On cancellation before acquisition: decrement refcount without releasing*
** Your code uses `ReleaseWithoutSemaphoreRelease` in the
   `OperationCanceledException` path.

* *Be strict about ownership*
** Only release the semaphore if it was *actually acquired*
** That’s the distinction between:
*** `Release()`
*** `ReleaseWithoutSemaphoreRelease()`

=== Why this matters

Cancellation is normal in real systems (timeouts, shutdowns, aborts).
If it corrupts bookkeeping, you’ll see:

* memory leaks,
* growing dictionary contention,
* mysterious performance degradation over days or weeks.

== Avoiding global contention and scaling with many keys

=== The challenge

A keyed lock reduces contention vs a global lock — unless you accidentally:

* lock the whole dictionary on every operation,
* use heavy per-key critical sections,
* allocate excessively.

=== Best practices

* *Use a concurrent dictionary; keep locks per-key*
** `ConcurrentDictionary<TKey, …>`
** `Monitor.Enter(releaser)` only for refcount/state updates

* *Keep per-key critical sections tiny*
** In `Release`, counters and dictionary removal are done inside the monitor
** `SemaphoreSlim.Release()` is done *outside* the critical section
   (important to avoid wake-ups while holding locks)

* *Choose an appropriate key comparer*
** `StringComparer.OrdinalIgnoreCase` avoids duplicate locks for
   case-different keys (important for file paths / IDs)

=== Why this matters

Keyed locks are used in *hot paths* (IO pipelines, caching, dedupe).
If the lock manager becomes the bottleneck, you lose the optimization entirely.

== Lifetime management: cleanup, disposal, and “too many keys”

=== The challenge

If keys are unbounded (request IDs, blob URLs, user IDs):

* you can create a semaphore per key indefinitely.

Even with refcount-based removal:

* refcount bugs ⇒ leaks,
* high churn ⇒ allocation pressure,
* long waits ⇒ entries live longer than expected.

=== Best practices

* *Refcount-based removal is the baseline*
** You already remove on last release.

* *Pooling (advanced, risky)*
** Reduces allocations
** Introduces ABA risks and stale state reuse
** Requires strict reset and ownership rules

* *Optional TTL / eviction for extreme cardinality*
** Prevents pathological memory growth
** Accepts occasional re-creation cost

=== Why this matters

In production:

----
high-cardinality keys + long-running service
= “works in tests” → fails after a week due to memory/GC pressure
----

== Fairness and starvation: hot keys

=== The challenge

`SemaphoreSlim` is *not strictly fair*. Under heavy contention:

* some waiters may starve,
* cancellations/timeouts cause churn,
* throughput degrades.

=== Best practices

* *Keep per-key concurrency = 1 unless needed*
** You use `m_maxCount = 1`.

* *Always support cancellation/timeouts*
** Prevents infinite buildup of waiters.

* *Use queued locks only if fairness is required*
** FIFO queues using `TaskCompletionSource`
** Higher complexity — only worth it if starvation is observed.

=== Why this matters

Starvation doesn’t throw exceptions.
It shows up as *random latency spikes* and stuck operations on hot keys.

== “Correct release” discipline: double-release & mis-ownership

=== The challenge

Callers might:

* dispose twice,
* dispose without owning,
* forget to dispose.

Any of these can corrupt concurrency.

=== Best practices

* *Return a releaser token and force `using / await using`*
[source,csharp]
----
await using var _ = await locker.LockAsync(key, ct);
----

* *Optional hardening*: idempotent dispose
** Add a disposed flag in the releaser
** Prevents double-release bugs

=== Why this matters

Keyed locks protect correctness-critical resources.
A single double-release can silently break exclusivity.

== Async vs sync boundaries: mixing Monitor with await

=== The challenge

Holding a synchronous lock across `await` = deadlock risk.

=== Best practices

* Use sync locks only for tiny bookkeeping
* Do async waiting on `SemaphoreSlim` outside locks

✔ Your design:
[source]
----
Monitor → refcount/state
await SemaphoreSlim.WaitAsync(...) → outside critical sections
----

=== Why this matters

This is the difference between:

* a safe async lock manager, and
* a deadlock generator.

== Preventing Resurrection of “Dead” Lock Objects (The Retry Loop Challenge)

=== The challange

In a keyed-lock implementation, a dictionary entry (the per-key lock object) can be in a *transitional state*:

- It still exists in the dictionary
- But it is in the process of being released and removed
- Or it has already been logically invalidated (`IsNotInUse = true`)

At this moment, another thread may:
- successfully read the entry from the dictionary
- but *must not reuse it*

If a caller reuses such an entry, the system may:
- wait on or release a semaphore that is already considered dead
- break reference counting
- accidentally allow multiple semaphores to exist for the same key over time

This leads to *silent loss of mutual exclusion*, which is one of the most dangerous concurrency bugs.

---

=== Why a simple `GetOrAdd` is not enough

A naïve implementation might assume:

____
If I got an entry from the dictionary, it must be safe to use.
____

This assumption is *false* under concurrency.

Between:
- retrieving the entry via `GetOrAdd`
- and incrementing the reference count

another thread may:
- decrement the reference count to zero
- remove the entry from the dictionary
- mark it as no longer usable

At that point, the object is *visible but invalid*.

---

=== The retry loop

[source,csharp]
----
while (true)
{
    releaserNoPooling = GetOrAdd(key, releaserToAddNoPooling);
    if (ReferenceEquals(releaserNoPooling, releaserToAddNoPooling)
        || releaserNoPooling.TryIncrement())
    {
        return releaserNoPooling;
    }
}
----

This loop ensures that the caller only proceeds when it has *safely claimed ownership* of a valid per-key lock object.

---

=== What the loop guarantees

Each iteration enforces one of two safe outcomes.

==== 1. We successfully inserted a new lock object

[source,csharp]
----
ReferenceEquals(releaserNoPooling, releaserToAddNoPooling)
----

This means:

- the dictionary accepted *our* newly created object
- it starts with `ReferenceCount = 1`
- it is not marked as `IsNotInUse`

This object is guaranteed to be valid.

---

==== 2. We successfully claimed an existing lock object

[source,csharp]
----
releaserNoPooling.TryIncrement()
----

This means:

- we acquired the per-object monitor
- we verified `IsNotInUse == false`
- we atomically incremented `ReferenceCount`

As a result:

- the entry cannot be removed while we are using or waiting on it
- ownership is clearly established

---

==== If neither condition holds

- The entry is being removed, *or*
- it has already been logically invalidated

In this case, the loop retries until:

- a fresh object is created, or
- a valid existing object can be safely claimed

---

=== Why this is critical

This retry loop prevents:

- resurrection of logically dead lock objects
- use-after-remove bugs
- reference count corruption
- multiple active semaphores for the same key
- silent loss of mutual exclusion

In effect, it makes the keyed-lock acquisition *linearizable* from the caller’s perspective.

---

=== Key takeaway

____
Seeing a lock object is not the same as safely owning it.
____

The retry loop is the mechanism that bridges this gap and makes the keyed-lock implementation correct under extreme concurrency.



== Specific topics tied to your challenge

Here are some keywords you can search/read that directly match the challenge you’re solving:

- “Safe publication / safe initialization under concurrency”
- “Reference counting in concurrent systems”
- “Object resurrection / ABA problem”
- “Linearizability vs. serializability”
- “Concurrent hash map entry lifecycle”
- “Per-key lock manager patterns”
- “Cancellation + resource cleanup”
- “Race between creation and removal”

== Testing async code

TaskCompletionSource concept:

----
“I’ll give you a Task now, and I will decide later when it finishes, fails, or gets canceled.”
----

[source,csharp]
----
var tcs = new TaskCompletionSource<int>();

Task<int> task = tcs.Task; // hand this to consumers

// later, from ANY thread:
tcs.SetResult(42); // TrySetResult         // completes the task successfully
// or:
tcs.SetException(ex);                      // faults the task
// or:
tcs.SetCanceled();                         // cancels the task
----
